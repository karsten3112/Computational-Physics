{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, \"../../\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from classes.nn_classes import train_model, NNmodel\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterDataset(Dataset):\n",
    "    def __init__(self, data_from_file, label_index=1) -> None:\n",
    "        super().__init__()\n",
    "        self.data = torch.tensor(np.array([data[0] for data in data_from_file]),dtype=torch.float32)\n",
    "        self.labels = torch.tensor(np.array([data[label_index] for data in data_from_file]),dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat = np.loadtxt('../pos_e1_e2_e3_1000.txt')\n",
    "data_in_file = [[d[:24].reshape(12,2),d[24],d[25],d[26]] for d in data_flat]\n",
    "\n",
    "ENERGY_LABEL = 2\n",
    "\n",
    "dataset_train = ClusterDataset(data_in_file[0:799], label_index=ENERGY_LABEL)\n",
    "dataset_val = ClusterDataset(data_in_file[799:899], label_index=ENERGY_LABEL)\n",
    "dataset_test = ClusterDataset(data_in_file[899:999], label_index=ENERGY_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LJGNN(torch.nn.Module):\n",
    "    def __init__(self, eps=0.5, sigma_squared=0.03, r0=1.6, eInf=0.1, A=0.9) -> None:\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.tensor(float(A)))\n",
    "        self.eps = torch.nn.Parameter(torch.tensor(float(eps)))\n",
    "        self.r0 = torch.nn.Parameter(torch.tensor(float(r0)))\n",
    "        self.sigma_squared = torch.nn.Parameter(torch.tensor(float(sigma_squared)))\n",
    "        self.epsinf = torch.nn.Parameter(torch.tensor(float(eInf)))\n",
    "    \n",
    "    def _V(self, r):\n",
    "        t1 = self.eps*torch.exp(-(r - self.r0)**2/(2.0*self.sigma_squared))\n",
    "        t2 = (1.0/r)**12 - 2.0*(1.0/r)**6\n",
    "        return self.A*(t2-t1) + self.epsinf\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        pairwise_distances = torch.stack([torch.pdist(coords[i]) for i in range(coords.size(0))])\n",
    "        return torch.sum(self._V(pairwise_distances), dim=1)\n",
    "\n",
    "class Pot_NN(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers=4, dim_hiddens=15, activation_func=torch.nn.Softplus(), bias=True) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_layers = torch.nn.ParameterList()\n",
    "        self.input_layer = torch.nn.Linear(1, dim_hiddens, bias=bias, dtype=torch.float)\n",
    "        self.output_layer = torch.nn.Linear(dim_hiddens, 1, bias=bias, dtype=torch.float)\n",
    "        self.activation_func = activation_func\n",
    "        for num in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(torch.nn.Linear(dim_hiddens, dim_hiddens, bias=bias, dtype=torch.float))\n",
    "\n",
    "    def _V(self, r):\n",
    "        pot = self.activation_func(self.input_layer(r))\n",
    "        for layer in self.hidden_layers:\n",
    "            pot = self.activation_func(layer(pot))\n",
    "        return self.output_layer(pot)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        pairwise_distances = torch.stack([torch.pdist(coords[i]) for i in range(coords.size(0))]).unsqueeze(-1)\n",
    "        #print(torch.sum(self._V(pairwise_distances), dim=1).view(-1))\n",
    "        return torch.sum(self._V(pairwise_distances), dim=1).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset_train, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(dataset=dataset_val, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(dataset=dataset_test, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m      7\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     train_losses, val_losses, test_loss \u001b[38;5;241m=\u001b[39m train_model(epochs\u001b[38;5;241m=\u001b[39mepochs, model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m      9\u001b[0m                                                       train_loader\u001b[38;5;241m=\u001b[39mtrain_loader, \n\u001b[0;32m     10\u001b[0m                                                       val_loader\u001b[38;5;241m=\u001b[39mval_loader, \n\u001b[0;32m     11\u001b[0m                                                       optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     12\u001b[0m                                                       test_loader\u001b[38;5;241m=\u001b[39mtest_loader, \n\u001b[0;32m     13\u001b[0m                                                       early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m                                                       patience\u001b[38;5;241m=\u001b[39mpatience)\n\u001b[0;32m     15\u001b[0m     model_losses\u001b[38;5;241m.\u001b[39mappend([train_losses, val_losses, test_loss])\n",
      "File \u001b[1;32mc:\\Users\\karst\\Computational-Physics\\Week10\\exercise 54\\../..\\classes\\nn_classes.py:65\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(epochs, model, train_loader, val_loader, optimizer, test_loader, early_stopping, patience, loss_func)\u001b[0m\n\u001b[0;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(model\u001b[38;5;241m.\u001b[39mforward(x_dat), y_dat) \u001b[38;5;66;03m#Remember that the loss calculates a mean loss over all points in batch here.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     train_losses[epoch] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mx_dat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m---> 65\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m train_losses[epoch]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[1;32mc:\\Users\\karst\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\karst\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 8000\n",
    "patience= 1000\n",
    "models = [Pot_NN(num_hidden_layers=4, dim_hiddens=16), Pot_NN(num_hidden_layers=4, dim_hiddens=16), LJGNN()]\n",
    "\n",
    "model_losses = []\n",
    "for model in models:\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "    train_losses, val_losses, test_loss = train_model(epochs=epochs, model=model, \n",
    "                                                      train_loader=train_loader, \n",
    "                                                      val_loader=val_loader, \n",
    "                                                      optimizer=optimizer,\n",
    "                                                      test_loader=test_loader, \n",
    "                                                      early_stopping=True,\n",
    "                                                      patience=patience)\n",
    "    model_losses.append([train_losses, val_losses, test_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(model_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
